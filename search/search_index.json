{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"setup/","text":"setup 1. Raspberry Pi \u69cb\u7bc9 ~ OS Setup","title":"setup"},{"location":"setup/#setup","text":"","title":"setup"},{"location":"setup/#1-raspberry-pi-os-setup","text":"","title":"1. Raspberry Pi \u69cb\u7bc9 ~ OS Setup"},{"location":"setup/01_setup_RaspberryPi/","text":"Raspberry Pi \u69cb\u6210 \u8cfc\u5165\u3057\u305f\u3082\u306e Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"01. Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#raspberry-pi","text":"","title":"Raspberry Pi \u69cb\u6210"},{"location":"setup/01_setup_RaspberryPi/#_1","text":"Raspberry Pi 4 Model B/2GB x3 GeeekPi Raspberry Pi4 \u30af\u30e9\u30b9\u30bf\u30fc\u30b1\u30fc\u30b9(\u51b7\u5374\u30d5\u30a1\u30f3\u4ed8\u304d) x1 Anker PowerPort I PD - 1 PD & 4 PowerIQ x1 Amazon\u30d9\u30fc\u30b7\u30c3\u30af HDMI\u30b1\u30fc\u30d6\u30eb 0.9m (\u30bf\u30a4\u30d7A\u30aa\u30b9 - \u30de\u30a4\u30af\u30ed\u30bf\u30a4\u30d7D\u30aa\u30b9) x1 Samsung EVO Plus 32GB microSDHC x3 Anker USB Type C \u30b1\u30fc\u30d6\u30eb PowerLine USB-C & USB-A 3.0 \u30b1\u30fc\u30d6\u30eb x3","title":"\u8cfc\u5165\u3057\u305f\u3082\u306e"},{"location":"setup/02_setup_ubuntu20-04-LTS/","text":"OS Setup UbuntuServer20.04.2LTS Setup GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.10.50/24 gateway4: 192.168.10.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 \u30db\u30b9\u30c8\u540d e.g. k8s-master name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts","title":"02. OS Install"},{"location":"setup/02_setup_ubuntu20-04-LTS/#os-setup","text":"","title":"OS Setup"},{"location":"setup/02_setup_ubuntu20-04-LTS/#ubuntuserver20042lts-setup","text":"GeeekPi\u30b1\u30fc\u30b9\u306e\u7d44\u307f\u7acb\u3066 & \u7d50\u7dda & \u8d77\u52d5\u78ba\u8a8d Raspberry Pi Imager \u3067SD\u30ab\u30fc\u30c9\u306bUbuntuServer20.04.2LTS(64bit) \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Raspberry Pi 4\u3078SD\u30ab\u30fc\u30c9\u3092\u633f\u5165\u3057\u8d77\u52d5 wifi\u8a2d\u5b9a sudo vim /etc/netplan/50-cloud-init.yaml # config check sudo netplan --debug try sudo netplan --debug generate # \u9069\u7528 sudo netplan --debug apply /etc/netplan/50-cloud-init.yaml (master\u306e\u5834\u5408) network: ethernets: eth0: dhcp4: true optional: true version: 2 wifis: wlan0: optional: true dhcp4: false addresses: - 192.168.10.50/24 gateway4: 192.168.10.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 search: [] access-points: \"<SSID\u540d>\": password: \"<\u30d1\u30b9\u30ef\u30fc\u30c9>\" package\u66f4\u65b0 sudo apt update sudo apt upgrade -y \u65e5\u672c\u8a9e\u30ad\u30fc\u30dc\u30fc\u30c9\u306b\u5909\u66f4\u3057\u518d\u8d77\u52d5 sudo dpkg-reconfigure keyboard-configuration sudo reboot Generic 105-key (Intl) PC \u3092\u9078\u629e Japanese \u3092\u9078\u629e Japanese \u3092\u9078\u629e The default for the keyboard layout \u3092\u9078\u629e No compose key \u3092\u9078\u629e LOCALE sudo apt install -y language-pack-ja sudo update-locale LANG=ja_JP.UTF-8 \u30db\u30b9\u30c8\u540d e.g. k8s-master name=k8s-master echo ${name} | sudo tee /etc/hostname sudo sed -i -e 's/127.0.1.1.*/127.0.1.1\\t'$name'/' /etc/hosts","title":"UbuntuServer20.04.2LTS Setup"},{"location":"setup/03_common_settings/","text":"1. swap\u3092\u7121\u52b9\u306b\u3059\u308b swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file. 2. cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1 3. CRI-O \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.20/xUbuntu_20.04/arm64/ VERSION=1.20 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio 4. CLI TOOL(buildah, cri-tools) buildah https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.20.0\" curl -L https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-arm.tar.gz --output crictl-${VERSION}-linux-arm.tar.gz sudo tar zxvf crictl-$VERSION-linux-arm.tar.gz -C /usr/local/bin rm -f crictl-$VERSION-linux-arm.tar.gz podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist 5. CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/containernetworking/plugins CNI_VERSION=\"v0.9.1\" ARCH=\"arm\" curl -L \"https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\" | sudo tar -C /opt/cni/bin -xz ls -l /opt/cni/bin/ 6. kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"03. master/node\u3067\u5171\u901a\u624b\u9806"},{"location":"setup/03_common_settings/#1-swap","text":"swap\u304c\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d $ free -h total used free shared buff/cache available Mem: 1.8Gi 54Mi 1.5Gi 8.0Mi 244Mi 1.7Gi Swap: 99Mi 0B 99Mi swap\u3092\u7121\u52b9\u306b\u8a2d\u5b9a\u3059\u308b sudo swapoff --all sudo systemctl stop dphys-swapfile sudo systemctl disable dphys-swapfile swap\u304c\u7121\u52b9\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b $ free -h total used free shared buff/cache available Mem: 1.8Gi 57Mi 1.5Gi 8.0Mi 251Mi 1.7Gi Swap: 0B 0B 0B $ systemctl status dphys-swapfile \u25cf dphys-swapfile.service - dphys-swapfile - set up, mount/unmount, and delete a swap file Loaded: loaded (/lib/systemd/system/dphys-swapfile.service; disabled; vendor preset: enabled) Active: inactive (dead) Docs: man:dphys-swapfile(8) 12\u6708 30 20:48:54 k8s-master1 systemd[1]: Starting dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 30 20:48:55 k8s-master1 dphys-swapfile[330]: want /var/swap=100MByte, checking existing: keeping it 12\u6708 30 20:48:55 k8s-master1 systemd[1]: Started dphys-swapfile - set up, mount/unmount, and delete a swap file. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopping dphys-swapfile - set up, mount/unmount, and delete a swap file... 12\u6708 31 06:57:57 k8s-master1 systemd[1]: dphys-swapfile.service: Succeeded. 12\u6708 31 06:57:57 k8s-master1 systemd[1]: Stopped dphys-swapfile - set up, mount/unmount, and delete a swap file.","title":"1. swap\u3092\u7121\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#2-cgroupfs-memory","text":"kernel\u306eboot option\u306b cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory \u3092\u8ffd\u8a18\u3059\u308b sudo vim /boot/firmware/cmdline.txt cmdline.txt \u306e\u6709\u52b9\u884c\u306e\u78ba\u8a8d $ sudo sed -e 's/\\s/\\n/g' /boot/firmware/cmdline.txt console=serial0,115200 console=tty1 root=PARTUUID=fb7271c3-02 rootfstype=ext4 elevator=deadline fsck.repair=yes rootwait quiet splash plymouth.ignore-serial-consoles cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory $ sudo reboot $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 9 1 1 cpu 5 34 1 cpuacct 5 34 1 blkio 10 34 1 memory 8 80 1 devices 4 34 1 freezer 7 1 1 net_cls 2 1 1 perf_event 6 1 1 net_prio 2 1 1 pids 3 39 1","title":"2. cgroupfs \u306ememory\u3092\u6709\u52b9\u306b\u3059\u308b"},{"location":"setup/03_common_settings/#3-cri-o","text":"https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o kernel module\u306eload overlay\u30d5\u30a1\u30a4\u30eb\u30b7\u30b9\u30c6\u30e0\u3092\u5229\u7528\u3059\u308b\u305f\u3081\u306ekernel module overlay iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306ekernel module br_netfilter cat <<EOF | sudo tee /etc/modules-load.d/crio.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter kernel parameter\u306eset iptables\u304cbridge\u3092\u901a\u904e\u3059\u308b\u30d1\u30b1\u30c3\u30c8\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u8a2d\u5b9a cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF sudo sysctl --system system\u8d77\u52d5\u6642\u306b kernel parameter \u3092\u518d\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b kube-proxy\u306b\u3066\u5fc5\u8981\u306akernel parameter\u8a2d\u5b9a(kubelet\u8a2d\u5b9a\u624b\u9806\u306b\u3066\u5f8c\u8ff0) \u304ciptables\u8d77\u52d5\u6642\u306ekernel module load\u3067\u4e0a\u66f8\u304d\u3055\u308c\u308b\u305f\u3081 \u5229\u7528\u74b0\u5883\u304c /etc/sysconfig/iptables-config \u3092\u5229\u7528\u53ef\u80fd\u306a\u3089 IPTABLES_MODULES_UNLOAD=\"no\" \u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u672c\u624b\u9806\u306f\u4e0d\u8981\u3067\u3059 egrep \"sysctl\\s+--system\" /etc/rc.local > /dev/null || sudo bash -c \"echo \\\"sysctl --system\\\" >> /etc/rc.local\" egrep \"sysctl\\s+--system\" /etc/rc.local CRI-O\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.20/xUbuntu_20.04/arm64/ VERSION=1.20 OS=xUbuntu_20.04 sudo bash -c \"echo \\\"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\" sudo bash -c \"echo \\\"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\\\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\" curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key add - curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key add - sudo apt update sudo apt install -y cri-o cri-o-runc sudo apt install -y conntrack storage driver\u3092 overlay2 \u3078\u5909\u66f4\u3059\u308b sudo vim /etc/containers/storage.conf sudo vim /etc/crio/crio.conf /etc/crio/crio.conf \u3078graph driver\u8a2d\u5b9a\u3092\u5165\u308c\u308b podman\u3084buildah\u3067build\u3057\u305flocal image\u3092\u53c2\u7167\u3059\u308b\u305f\u3081 [crio] \u30bb\u30af\u30b7\u30e7\u30f3\u306b\u5165\u308c\u308b graphroot = \"/var/lib/containers/storage\" /etc/containers/storage.conf [storage] driver = \"overlay2\" runroot = \"/run/containers/storage\" graphroot = \"/var/lib/containers/storage\" [storage.options] additionalimagestores = [ ] [storage.options.overlay] mountopt = \"nodev\" [storage.options.thinpool] /etc/crio/crio.conf [crio] storage_driver = \"overlay2\" graphroot = \"/var/lib/containers/storage\" log_dir = \"/var/log/crio/pods\" version_file = \"/var/run/crio/version\" version_file_persist = \"/var/lib/crio/version\" clean_shutdown_file = \"/var/lib/crio/clean.shutdown\" [crio.api] listen = \"/var/run/crio/crio.sock\" stream_address = \"127.0.0.1\" stream_port = \"0\" stream_enable_tls = false stream_idle_timeout = \"\" stream_tls_cert = \"\" stream_tls_key = \"\" stream_tls_ca = \"\" grpc_max_send_msg_size = 16777216 grpc_max_recv_msg_size = 16777216 [crio.runtime] no_pivot = false decryption_keys_path = \"/etc/crio/keys/\" conmon = \"\" conmon_cgroup = \"system.slice\" conmon_env = [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", ] default_env = [ ] seccomp_profile = \"\" seccomp_use_default_when_empty = false apparmor_profile = \"crio-default\" irqbalance_config_file = \"/etc/sysconfig/irqbalance\" cgroup_manager = \"systemd\" separate_pull_cgroup = \"\" default_capabilities = [ \"CHOWN\", \"DAC_OVERRIDE\", \"FSETID\", \"FOWNER\", \"SETGID\", \"SETUID\", \"SETPCAP\", \"NET_BIND_SERVICE\", \"KILL\", ] default_sysctls = [ ] additional_devices = [ ] hooks_dir = [ \"/usr/share/containers/oci/hooks.d\", ]pids_limit = 1024 log_size_max = -1 log_to_journald = false container_exits_dir = \"/var/run/crio/exits\" container_attach_socket_dir = \"/var/run/crio\" bind_mount_prefix = \"\" read_only = false log_level = \"info\" log_filter = \"\" uid_mappings = \"\" gid_mappings = \"\" ctr_stop_timeout = 30 drop_infra_ctr = false infra_ctr_cpuset = \"\" namespaces_dir = \"/var/run\" pinns_path = \"\" default_runtime = \"runc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" allowed_annotations = [ \"io.containers.trace-syscall\", ] [crio.image] default_transport = \"docker://\" global_auth_file = \"\" pause_image = \"k8s.gcr.io/pause:3.2\" pause_image_auth_file = \"\" pause_command = \"/pause\" signature_policy = \"\" image_volumes = \"mkdir\" big_files_temporary_dir = \"\" [crio.network] network_dir = \"/etc/cni/net.d/\" plugin_dirs = [ \"/opt/cni/bin/\", ] [crio.metrics] enable_metrics = false metrics_port = 9090 metrics_socket = \"\" crio\u3092\u518d\u8d77\u52d5\u3059\u308b sudo systemctl daemon-reload sudo systemctl restart crio","title":"3. CRI-O \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#4-cli-toolbuildah-cri-tools","text":"buildah https://github.com/containers/buildah/blob/master/install.md sudo apt-get -qq -y install buildah cri-tools(crictl) \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md VERSION=\"v1.20.0\" curl -L https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-arm.tar.gz --output crictl-${VERSION}-linux-arm.tar.gz sudo tar zxvf crictl-$VERSION-linux-arm.tar.gz -C /usr/local/bin rm -f crictl-$VERSION-linux-arm.tar.gz podman \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb(optional) https://podman.io/getting-started/installation sudo apt-get -y install podman sudo rm -f /etc/cni/net.d/87-podman-bridge.conflist","title":"4. CLI TOOL(buildah, cri-tools)"},{"location":"setup/03_common_settings/#5-cni-plugin","text":"https://github.com/containernetworking/plugins CNI_VERSION=\"v0.9.1\" ARCH=\"arm\" curl -L \"https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\" | sudo tar -C /opt/cni/bin -xz ls -l /opt/cni/bin/","title":"5. CNI Plugin\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/03_common_settings/#6-kubectl","text":"https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/","title":"6. kubectl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/","text":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u624b\u9806 cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install golang-cfssl CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210 cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem \u8a3c\u660e\u66f8\u306e\u4f5c\u6210 \u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8 cat > admin-csr.json <<EOF { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 EXTERNAL_IP master\u30b5\u30fc\u30d0\u306eIP\u30a2\u30c9\u30ec\u30b9 master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat > ${instance}-csr.json <<EOF { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=192.168.10.50 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat > kube-proxy-csr.json <<EOF { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat > kube-controller-manager-csr.json <<EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8 cat > kube-scheduler-csr.json <<EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8 10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat > kubernetes-csr.json <<EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,192.168.10.50,192.168.10.51,192.168.10.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem service-account\u306e\u8a3c\u660e\u66f8 cat > service-account-csr.json <<EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem \u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b master node \u53c2\u8003\u6587\u732e https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"04. \u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#tls","text":"","title":"\u8a8d\u8a3c\u5c40\u306e\u8a2d\u5b9a\u3068TLS\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/04_creation_certificate/#cfssl","text":"\u8a3c\u660e\u66f8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e cfssl \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b https://qiita.com/iaoiui/items/fc2ea829498402d4a8e3 https://coreos.com/os/docs/latest/generate-self-signed-certificates.html sudo apt install golang-cfssl","title":"cfssl \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"setup/04_creation_certificate/#ca","text":"cat > ca-config.json <<EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF cat > ca-csr.json <<EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"CA\", \"ST\": \"Sample\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d ca-key.pem ca.pem","title":"CA(\u8a8d\u8a3c\u5c40) \u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_2","text":"","title":"\u8a3c\u660e\u66f8\u306e\u4f5c\u6210"},{"location":"setup/04_creation_certificate/#_3","text":"cat > admin-csr.json <<EOF { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:masters\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d admin-key.pem admin.pem","title":"\u7ba1\u7406\u8005\u30e6\u30fc\u30b6 \u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kubelet","text":"EXTERNAL_IP master\u30b5\u30fc\u30d0\u306eIP\u30a2\u30c9\u30ec\u30b9 master\u304c\u8907\u6570\u30b5\u30fc\u30d0\u69cb\u6210\u306e\u5834\u5408\u306f\u4e0a\u4f4d\u306eLB IP for instance in k8s-master k8s-node1 k8s-node2; do cat > ${instance}-csr.json <<EOF { \"CN\": \"system:node:${instance}\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:nodes\", \"OU\": \"Kubernetes The HardWay\", \"ST\": \"Sample\" } ] } EOF EXTERNAL_IP=192.168.10.50 cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d k8s-node1-key.pem k8s-node1.pem k8s-node2-key.pem k8s-node2.pem","title":"kubelet\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-proxy","text":"cat > kube-proxy-csr.json <<EOF { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:node-proxier\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-proxy-key.pem kube-proxy.pem","title":"kube-proxy\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-controller-manage","text":"cat > kube-controller-manager-csr.json <<EOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"system:kube-controller-manager\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-controller-manager-key.pem kube-controller-manager.pem","title":"kube-controller-manage\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-scheduler","text":"cat > kube-scheduler-csr.json <<EOF { \"CN\": \"system:kube-scheduler\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"US\", \"L\": \"Portland\", \"O\": \"system:kube-scheduler\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kube-scheduler-key.pem kube-scheduler.pem","title":"kube-scheduler\u306e\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#kube-apiserver","text":"10.32.0.1 Cluster IP KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local cat > kubernetes-csr.json <<EOF { \"CN\": \"Kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=10.32.0.1,192.168.10.50,192.168.10.51,192.168.10.52,127.0.0.1,${KUBERNETES_HOSTNAMES} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d kubernetes-key.pem kubernetes.pem","title":"kube-apiserver\u306e\u30b5\u30fc\u30d0\u30fc\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#service-account","text":"cat > service-account-csr.json <<EOF { \"CN\": \"service-accounts\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"JP\", \"L\": \"Tokyo\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes The Hard Way\", \"ST\": \"Sample\" } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account \u4ee5\u4e0b\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d service-account-key.pem service-account.pem","title":"service-account\u306e\u8a3c\u660e\u66f8"},{"location":"setup/04_creation_certificate/#masternode","text":"master node","title":"\u8a3c\u660e\u66f8\u3092master/node\u3078\u30b3\u30d4\u30fc\u3059\u308b"},{"location":"setup/04_creation_certificate/#_4","text":"https://kubernetes.io/ja/docs/setup/best-practices/certificates/ https://kubernetes.io/ja/docs/concepts/cluster-administration/certificates/ https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/05_creating_config/","text":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210 Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b \u624b\u9806 kubelet KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done kube-proxy KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig kube-controller-manager KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig kube-scheduler KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig admin KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig \u53c2\u8003\u8cc7\u6599 https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"05. \u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#kubeconfig","text":"Controll Plane\u3068Node\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e .kubeconfig \u3092\u4f5c\u6210\u3059\u308b","title":"\u8a8d\u8a3c\u306e\u305f\u3081\u306ekubeconfig\u306e\u4f5c\u6210"},{"location":"setup/05_creating_config/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/05_creating_config/#kubelet","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 for instance in k8s-master k8s-node1 k8s-node2; do kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done","title":"kubelet"},{"location":"setup/05_creating_config/#kube-proxy","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig","title":"kube-proxy"},{"location":"setup/05_creating_config/#kube-controller-manager","text":"KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig","title":"kube-controller-manager"},{"location":"setup/05_creating_config/#kube-scheduler","text":"KUBE_API_SERVER_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBE_API_SERVER_ADDRESS}:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig","title":"kube-scheduler"},{"location":"setup/05_creating_config/#admin","text":"KUBERNETES_PUBLIC_ADDRESS=192.168.10.50 kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig","title":"admin"},{"location":"setup/05_creating_config/#_2","text":"https://github.com/kelseyhightower/kubernetes/blob/master/docs/05-kubernetes-configuration-files.md https://docs.oracle.com/cd/F34086_01/kubernetes-on-oci_jp.pdf https://h3poteto.hatenablog.com/entry/2020/08/20/180552 kubectl config set-cluster https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-cluster/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-cluster-em- kubectl config set-credentials https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-credentials/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-credentials-em- kubectl config set-context https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_config_set-context/ https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-set-context-em-","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/bootstrapping_kubelet/","text":"bootstrapping kubelet(master/worker \u5171\u901a) \u624b\u9806 kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 sudo wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kubelet sudo chmod +x /usr/bin/kubelet config, \u8a3c\u660e\u66f8\u306a\u3069\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 500m memory: 300Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 100m memory: 100Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat <<'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service \u30a8\u30e9\u30fc\u4e8b\u4f8b cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408 kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice] cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f \u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b 360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044 \u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal. Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044 I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044 kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases Kubelet cannot determine CPU online state sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039 \u53c2\u8003 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"01. bootstrapping kubelet"},{"location":"setup/bootstrapping_kubelet/#bootstrapping-kubeletmasterworker","text":"","title":"bootstrapping kubelet(master/worker \u5171\u901a)"},{"location":"setup/bootstrapping_kubelet/#_1","text":"kubelet \u30d0\u30a4\u30ca\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 sudo wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kubelet sudo chmod +x /usr/bin/kubelet config, \u8a3c\u660e\u66f8\u306a\u3069\u3092\u914d\u7f6e # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" sudo install -o root -g root -m 755 -d /etc/kubelet.d sudo install -o root -g root -m 755 -d /var/lib/kubernetes sudo install -o root -g root -m 755 -d /var/lib/kubelet sudo cp ca.pem /var/lib/kubernetes/ sudo cp ${host}.pem ${host}-key.pem ${host}.kubeconfig /var/lib/kubelet/ sudo cp ${host}.kubeconfig /var/lib/kubelet/kubeconfig /var/lib/kubelet/kubelet-config.yaml \u3092\u4f5c\u6210\u3059\u308b clusterDNS \u306f kube-dns(core-dns)\u306eClusterIP\u3092\u6307\u5b9a\u3059\u308b podCIDR \u306fnode\u3067\u8d77\u52d5\u3059\u308bPod\u306b\u5272\u308a\u5f53\u3066\u308bIP\u30a2\u30c9\u30ec\u30b9\u306eCIDR\u3092\u6307\u5b9a\u3059\u308b # host=\"k8s-node2\" # host=\"k8s-node1\" host=\"k8s-master\" cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 # https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ staticPodPath: /etc/kubelet.d # kubelet\u306e\u8a8d\u8a3c\u65b9\u5f0f # - anonymous: false \u304c(\u30b3\u30f3\u30c6\u30ca\u5b9f\u884c\u30db\u30b9\u30c8\u306eHardening\u3068\u3057\u3066)\u63a8\u5968\u3055\u308c\u308b # - webhook.enabled: true \u306e\u5834\u5408\u306fkube-api-server\u5074\u3067\u3082\u8af8\u51e6\u306e\u8a2d\u5b9a\u304c\u5fc5\u8981 authentication: anonymous: enabled: true webhook: enabled: false cacheTTL: \"2m\" x509: clientCAFile: \"/var/lib/kubernetes/ca.pem\" # kubelet\u306e\u8a8d\u53ef\u8a2d\u5b9a # - authorization.mode \u306edefault\u52d5\u4f5c\u306f AlwaysAllow # - authorization.mode: Webhook \u306e\u5834\u5408\u306f kube-api-server\u3067 authorization.k8s.io/v1beta1 \u306e\u6709\u52b9\u8a2d\u5b9a\u304c\u5fc5\u8981 authorization: mode: AlwaysAllow clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" podCIDR: \"10.200.0.0/24\" resolvConf: \"/etc/resolv.conf\" runtimeRequestTimeout: \"15m\" tlsCertFile: \"/var/lib/kubelet/${host}.pem\" tlsPrivateKeyFile: \"/var/lib/kubelet/${host}-key.pem\" # Reserve Compute Resources for System Daemons # https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ # # Pod\u306b\u914d\u7f6e\u53ef\u80fd\u306a\u30ea\u30bd\u30fc\u30b9\u306f \"Node resource - system-reserved - kube-reserved - eviction-threshold\" \u3089\u3057\u3044 # # system-reserved # - OS system daemons(ssh, udev, etc) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b # # kube-reserved # - k8s system daemons(kubelet, container runtime, node problem detector) \u7528\u306b\u30ea\u30bd\u30fc\u30b9\u3092\u78ba\u4fdd\u3059\u308b enforceNodeAllocatable: [\"pods\",\"kube-reserved\",\"system-reserved\"] cgroupsPerQOS: true cgroupDriver: systemd cgroupRoot: / systemCgroups: /system.slice systemReservedCgroup: /system.slice systemReserved: cpu: 500m memory: 300Mi runtimeCgroups: /kube.slice/crio.service kubeletCgroups: /kube.slice/kubelet.service kubeReservedCgroup: /kube.slice kubeReserved: cpu: 100m memory: 100Mi EOF /etc/systemd/system/kubelet.service \u3092\u914d\u7f6e cat <<'EOF' | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=crio.service Requires=crio.service [Service] Restart=on-failure RestartSec=5 ExecStartPre=/usr/bin/mkdir -p \\ /sys/fs/cgroup/systemd/kube.slice \\ /sys/fs/cgroup/cpuset/kube.slice \\ /sys/fs/cgroup/cpuset/system.slice \\ /sys/fs/cgroup/pids/kube.slice \\ /sys/fs/cgroup/pids/system.slice \\ /sys/fs/cgroup/memory/kube.slice \\ /sys/fs/cgroup/memory/system.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice \\ /sys/fs/cgroup/cpu,cpuacct/kube.slice ExecStart=/usr/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --network-plugin=cni \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --register-node=true \\ --v=2 [Install] WantedBy=multi-user.target EOF kubelet.service \u3092\u8d77\u52d5 sudo systemctl enable kubelet.service sudo systemctl start kubelet.service","title":"\u624b\u9806"},{"location":"setup/bootstrapping_kubelet/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/bootstrapping_kubelet/#cgroup","text":"kubelet.go:1347] Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on \"/kube.slice\": [\"kube\"] cgroup does not exist kubelet \u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u8a73\u7d30\u306a\u30ed\u30b0\u3092\u51fa\u3059\u3053\u3068\u3067Path\u304c\u308f\u304b\u3063\u305f( --v 10 ) cgroup_manager_linux.go:294] The Cgroup [kube] has some missing paths: [/sys/fs/cgroup/pids/kube.slice /sys/fs/cgroup/memory/kube.slice]","title":"cgroup\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u672a\u4f5c\u6210\u306e\u5834\u5408"},{"location":"setup/bootstrapping_kubelet/#cgroupsystemreserved-memory-size","text":"\u539f\u56e0\u306a\u3069\u306f\u672a\u8abf\u67fb\u3001systemReserved memory\u3092\u5927\u304d\u304f\u3057\u305f\u3089\u767a\u751f\u3057\u306a\u304f\u306a\u3063\u305f kubelet.go:1347] Failed to start ContainerManager Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": failed to set supported cgroup subsystems for cgroup [system]: failed to set config for supported subsystems : failed to write \"104857600\" to \"/sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes\": write /sys/fs/cgroup/memory/system.slice/memory.limit_in_bytes: device or resource busy","title":"cgroup\u3067\u78ba\u4fdd\u3059\u308bsystemReserved memory size\u304c\u5c0f\u3055\u3044\u5834\u5408\u306b\u767a\u751f"},{"location":"setup/bootstrapping_kubelet/#kubeconfig-cn-node","text":"360163 kubelet_node_status.go:93] Unable to register node \"k8s-master\" with API server: nodes \"k8s-master\" is forbidden: node \"k8s-node1\" is not allowed to modify node \"k8s-master\" kubeconfig\u306eclient-certificate-data\u306eCN\u3092\u78ba\u8a8d\u3059\u308b sudo cat k8s-master.kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: k8s-master \u304c\u6b63\u3057\u3044\u306e\u306b CN = system:node:k8s-node1 \u3068\u306a\u3063\u3066\u3044\u305f root@k8s-master:~# cat /var/lib/kubelet/kubeconfig | grep client-certificate-data | awk '{print $2;}' | base64 -d | openssl x509 -text | grep Subject: Subject: C = JP, ST = Sample, L = Tokyo, O = system:nodes, OU = Kubernetes The HardWay, CN = system:node:k8s-master","title":"kubeconfig \u306e\u8a3c\u660e\u66f8\u306e CN \u304cnode \u30db\u30b9\u30c8\u540d\u3068\u7570\u306a\u308b"},{"location":"setup/bootstrapping_kubelet/#node-specpodcidr-cidr","text":"\u4ee5\u4e0b\u30b3\u30de\u30f3\u30c9\u3067node\u306b\u8a2d\u5b9a\u3057\u305fpodCIDR\u304c\u8868\u793a\u3055\u308c\u306a\u3044 flannnel\u304c\u8d77\u52d5\u3057\u306a\u3044\u539f\u56e0\u304c\u3053\u3053\u306b\u3042\u3063\u305f... kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' kube-controller-manager \u306e\u30ed\u30b0 Set node k8s-node1 PodCIDR to [10.200.0.0/24] \u304c\u51fa\u308b\u3053\u3068\u304c\u30dd\u30a4\u30f3\u30c8 kube-controller-manager \u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b --allocate-node-cidrs=true \u304c\u5fc5\u8981\u3063\u3066\u304a\u8a71... actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName=\"k8s-node1\" does not exist range_allocator.go:373] Set node k8s-node1 PodCIDR to [10.200.0.0/24] ttl_controller.go:276] \"Changed ttl annotation\" node=\"k8s-node1\" new_ttl=\"0s\" controller.go:708] Detected change in list of current cluster nodes. New node set: map[k8s-node1:{}] controller.go:716] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes node_lifecycle_controller.go:773] Controller observed a new Node: \"k8s-node1\" controller_utils.go:172] Recording Registered Node k8s-node1 in Controller event message for node k8s-node1 node_lifecycle_controller.go:1429] Initializing eviction metric for zone: node_lifecycle_controller.go:1044] Missing timestamp for Node k8s-node1. Assuming now as a timestamp. event.go:291] \"Event occurred\" object=\"k8s-node1\" kind=\"Node\" apiVersion=\"v1\" type=\"Normal\" reason=\"RegisteredNode\" message=\"Node k8s-node1 event: Registered Node k8s-node1 in Controller\" node_lifecycle_controller.go:1245] Controller detected that zone is now in state Normal.","title":"Node \u30ea\u30bd\u30fc\u30b9\u306e spec.podCIDR \u306bCIDR\u304c\u8a2d\u5b9a\u3055\u308c\u306a\u3044"},{"location":"setup/bootstrapping_kubelet/#webhook-authentication","text":"I0214 07:03:56.822586 1 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/var/lib/kubernetes/ca.pem\" F0214 07:03:56.822637 1 server.go:269] failed to run Kubelet: no client provided, cannot use webhook authentication goroutine 1 [running]: https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"Webhook Authentication\u306e\u8a2d\u5b9a\u304c\u6b63\u3057\u304f\u306a\u3044"},{"location":"setup/bootstrapping_kubelet/#cni-plugin-etccninetd-cni-plugin","text":"kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized cni.go:239] Unable to update cni config: no networks found in /etc/cni/net.d kubelet.go:2163] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized CNI Plugin\u3092 /etc/cni/net.d \u3078\u7f6e\u304f\u3053\u3068\u3067\u89e3\u6c7a\u3059\u308b https://github.com/containernetworking/plugins/releases","title":"CNI Plugin\u3092 /etc/cni/net.d \u3067CNI Plugin\u304c\u898b\u3064\u304b\u3089\u306a\u3044"},{"location":"setup/bootstrapping_kubelet/#kubelet-cannot-determine-cpu-online-state","text":"sysinfo.go:203] Nodes topology is not available, providing CPU topology sysfs.go:348] unable to read /sys/devices/system/cpu/cpu0/online: open /sys/devices/system/cpu/cpu0/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu1/online: open /sys/devices/system/cpu/cpu1/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu2/online: open /sys/devices/system/cpu/cpu2/online: no such file or directory sysfs.go:348] unable to read /sys/devices/system/cpu/cpu3/online: open /sys/devices/system/cpu/cpu3/online: no such file or directory gce.go:44] Error while reading product_name: open /sys/class/dmi/id/product_name: no such file or directory machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:72] Cannot read number of physical cores correctly, number of cores set to 0 machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu0 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu1 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu2 online state, skipping machine.go:253] Cannot determine CPU /sys/bus/cpu/devices/cpu3 online state, skipping machine.go:86] Cannot read number of sockets correctly, number of sockets set to 0 container_manager_linux.go:490] [ContainerManager]: Discovered runtime cgroups name: \u65e2\u77e5\u3089\u3057\u3044 https://github.com/kubernetes/kubernetes/issues/95039","title":"Kubelet cannot determine CPU online state"},{"location":"setup/bootstrapping_kubelet/#_3","text":"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go https://cyberagent.ai/blog/tech/4036/ kubelet \u306e\u8a2d\u5b9a\u3092\u5909\u66f4\u3057\u3066 runtime \u306b cri-o \u3092\u6307\u5b9a\u3059\u308b https://downloadkubernetes.com/ https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/ Node Authorization https://qiita.com/tkusumi/items/f6a4f9150aa77d8f9822 https://kubernetes.io/docs/reference/access-authn-authz/node/ https://kubernetes.io/ja/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ static pod https://kubernetes.io/ja/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/policy/pod-security-policy/ https://hakengineer.xyz/2019/07/04/post-1997/#03_master1kube-schedulerkube-controller-managerkube-apiserver PodSecurityPolicy \u3092\u53c2\u7167\u3057\u305f\u5143\u30cd\u30bf( false \u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f true \u306b\u76f4\u3059) https://github.com/kubernetes/kubernetes/issues/70952","title":"\u53c2\u8003"},{"location":"setup/master/01_bootstrapping_etcd/","text":"bootstrapping etcd coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002 \u624b\u9806 Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << EOF > Dockerfile_etcd.armhf FROM arm64v8/ubuntu:bionic AS etcd-builder RUN set -ex \\ && apt update \\ && apt install -y git wget tar zip \\ && apt clean RUN set -ex \\ && wget https://golang.org/dl/go1.15.6.linux-arm64.tar.gz \\ && tar -C /usr/local -xzf go1.15.6.linux-arm64.tar.gz RUN set -ex \\ && git clone https://github.com/etcd-io/etcd.git /tmp/etcd\\ && cd /tmp/etcd \\ && PATH=$PATH:/usr/local/go/bin:~/go/bin ./build FROM arm64v8/ubuntu:bionic COPY --from=etcd-builder /tmp/etcd/bin/etcd /usr/local/bin/ COPY --from=etcd-builder /tmp/etcd/bin/etcdctl /usr/local/bin/ RUN set -ex \\ && apt update \\ && apt clean \\ && install -o root -g root -m 700 -d /var/lib/etcd \\ && install -o root -g root -m 644 -d /etc/etcd COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo buildah bud -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.10.50:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: etcd image: localhost/k8s-etcd:latest imagePullPolicy: IfNotPresent env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: \"0.5\" memory: \"256Mi\" limits: cpu: \"1\" memory: \"384Mi\" command: - /usr/local/bin/etcd - --advertise-client-urls=https://192.168.10.50:2379,https://192.168.10.50:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://192.168.10.50:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://192.168.10.50:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a \u53c2\u8003\u6587\u732e https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"02. bootstrapping etcd"},{"location":"setup/master/01_bootstrapping_etcd/#bootstrapping-etcd","text":"coreos\u304cetcd docker image \u3092\u63d0\u4f9b \u3057\u3066\u3044\u307e\u3059\u304c\u3001Raspberry Pi\u306b\u642d\u8f09\u3055\u308c\u3066\u3044\u308bARM CPU\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3 armv8(64bit) \u3067\u5229\u7528\u53ef\u80fd\u306aimage\u306f\u63d0\u4f9b\u3057\u3066\u3044\u306a\u3044\u305f\u3081\u3001image\u3092build\u3057\u307e\u3059\u3002","title":"bootstrapping etcd"},{"location":"setup/master/01_bootstrapping_etcd/#_1","text":"Dockerfile_etcd.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_etcd.armhf cat << EOF > Dockerfile_etcd.armhf FROM arm64v8/ubuntu:bionic AS etcd-builder RUN set -ex \\ && apt update \\ && apt install -y git wget tar zip \\ && apt clean RUN set -ex \\ && wget https://golang.org/dl/go1.15.6.linux-arm64.tar.gz \\ && tar -C /usr/local -xzf go1.15.6.linux-arm64.tar.gz RUN set -ex \\ && git clone https://github.com/etcd-io/etcd.git /tmp/etcd\\ && cd /tmp/etcd \\ && PATH=$PATH:/usr/local/go/bin:~/go/bin ./build FROM arm64v8/ubuntu:bionic COPY --from=etcd-builder /tmp/etcd/bin/etcd /usr/local/bin/ COPY --from=etcd-builder /tmp/etcd/bin/etcdctl /usr/local/bin/ RUN set -ex \\ && apt update \\ && apt clean \\ && install -o root -g root -m 700 -d /var/lib/etcd \\ && install -o root -g root -m 644 -d /etc/etcd COPY ca.pem /etc/etcd/ COPY kubernetes-key.pem /etc/etcd/ COPY kubernetes.pem /etc/etcd/ ENV ETCD_UNSUPPORTED_ARCH=arm64 EXPOSE 2379 2380 ENTRYPOINT [\"/usr/local/bin/etcd\"] EOF image build sudo buildah bud -t k8s-etcd --file=Dockerfile_etcd.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/etcd.yaml cat << EOF | sudo tee /etc/kubelet.d/etcd.yaml --- apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.10.50:2379 name: etcd namespace: kube-system labels: tier: control-plane component: etcd spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: etcd image: localhost/k8s-etcd:latest imagePullPolicy: IfNotPresent env: - name: ETCD_UNSUPPORTED_ARCH value: \"arm64\" resources: requests: cpu: \"0.5\" memory: \"256Mi\" limits: cpu: \"1\" memory: \"384Mi\" command: - /usr/local/bin/etcd - --advertise-client-urls=https://192.168.10.50:2379,https://192.168.10.50:2380 - --listen-client-urls=https://0.0.0.0:2379 - --initial-advertise-peer-urls=https://192.168.10.50:2380 - --listen-peer-urls=https://0.0.0.0:2380 - --name=etcd0 - --cert-file=/etc/etcd/kubernetes.pem - --key-file=/etc/etcd/kubernetes-key.pem - --peer-cert-file=/etc/etcd/kubernetes.pem - --peer-key-file=/etc/etcd/kubernetes-key.pem - --trusted-ca-file=/etc/etcd/ca.pem - --peer-trusted-ca-file=/etc/etcd/ca.pem - --peer-client-cert-auth - --client-cert-auth - --initial-cluster-token=etcd-cluster-1 - --initial-cluster=etcd0=https://192.168.10.50:2380 - --initial-cluster-state=new EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name etcd CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 72f58248ec087 6e8b8110dc13cfe61d75f867a22c39766a397989413570500f51dedf94be7a12 25 seconds ago Running etcd 0 206c5b952097a","title":"\u624b\u9806"},{"location":"setup/master/01_bootstrapping_etcd/#_2","text":"https://etcd.io/docs/v2/docker_guide/ https://quay.io/repository/coreos/etcd?tag=latest&tab=tags https://github.com/etcd-io/etcd","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/master/02_bootstrapping_kube-apiserver/","text":"bootstrapping kube-apiserver \u624b\u9806 Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << EOF > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=192.168.10.50 cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo buildah bud -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-api-server.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: localhost/k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"0.5\" memory: \"256Mi\" limits: cpu: \"1\" memory: \"384Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=192.168.10.50 - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass,PodSecurityPolicy - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://192.168.10.50:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --kubelet-https=true - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: allowedCapabilities: [] # default set of capabilities are implicitly allowed allowPrivilegeEscalation: false fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}] \u30a8\u30e9\u30fc\u4e8b\u4f8b 1.20\u306e\u30d0\u30b0? failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request \u89e3\u6c7a\u65b9\u6cd5(2021/04/17\u6642\u70b9) https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408 failed to run Kubelet: no client provided, cannot use webhook authentication \u89e3\u6c7a\u65b9\u6cd5 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/","title":"03. bootstrapping kube-apiserver"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#bootstrapping-kube-apiserver","text":"","title":"bootstrapping kube-apiserver"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#_1","text":"Dockerfile_kube-apiserver.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-apiserver.armhf cat << EOF > Dockerfile_kube-apiserver.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget --quiet -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-apiserver \\ && chmod +x /usr/bin/kube-apiserver \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config \\ && install -o root -g root -m 755 -d /etc/kubernetes/webhook COPY ca.pem \\ ca-key.pem \\ kubernetes-key.pem \\ kubernetes.pem \\ service-account-key.pem \\ service-account.pem \\ encryption-config.yaml \\ /var/lib/kubernetes/ COPY authorization-config.yaml /etc/kubernetes/webhook/ EXPOSE 6443 ENTRYPOINT [\"/usr/bin/kube-apiserver\"] EOF encryption-provider-config \u3092\u4f5c\u6210\u3059\u308b --encryption-provider-config \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u6307\u5b9a\u3057\u3066secret\u30ea\u30bd\u30fc\u30b9\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u6697\u53f7\u5316\u3059\u308b\u305f\u3081\u306e\u9375\u3092\u5b9a\u7fa9\u3059\u308b https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#encrypting-your-data https://access.redhat.com/documentation/ja-jp/openshift_container_platform/3.11/html/cluster_administration/admin-guide-encrypting-data-at-datastore encryption-config.yaml ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat << EOF > encryption-config.yaml kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF webbhook config\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b --authorization-webhook-config-file \u3067\u6307\u5b9a\u3059\u308b\u30d5\u30a1\u30a4\u30eb authorization-config.yaml KUBE_API_SERVER_ADDRESS=192.168.10.50 cat << EOF > authorization-config.yaml --- apiVersion: v1 # kind of the API object kind: Config # clusters refers to the remote service. clusters: - name: kubernetes cluster: certificate-authority: /var/lib/kubernetes/ca.pem # CA for verifying the remote service. server: https://${KUBE_API_SERVER_ADDRESS}:6443/authenticate # URL of remote service to query. Must use 'https'. # users refers to the API server's webhook configuration. users: - name: api-server-webhook user: client-certificate: /var/lib/kubernetes/kubernetes.pem # cert for the webhook plugin to use client-key: /var/lib/kubernetes/kubernetes-key.pem # key matching the cert # kubeconfig files require a context. Provide one for the API server. current-context: webhook contexts: - context: cluster: kubernetes user: api-server-webhook name: webhook EOF image build sudo buildah bud -t k8s-kube-apiserver --file=Dockerfile_kube-apiserver.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-api-server.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-api-server.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default labels: tier: control-plane component: kube-apiserver spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-apiserver image: localhost/k8s-kube-apiserver:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"0.5\" memory: \"256Mi\" limits: cpu: \"1\" memory: \"384Mi\" command: - /usr/bin/kube-apiserver - --advertise-address=192.168.10.50 - --allow-privileged=true - --anonymous-auth=false - --apiserver-count=1 - --audit-log-maxage=30 - --audit-log-maxbackup=3 - --audit-log-maxsize=100 - --audit-log-path=/var/log/audit.log - --authorization-mode=Node,RBAC,Webhook - --authorization-webhook-config-file=/etc/kubernetes/webhook/authorization-config.yaml - --authentication-token-webhook-cache-ttl=2m - --authentication-token-webhook-version=v1 - --bind-address=0.0.0.0 - --client-ca-file=/var/lib/kubernetes/ca.pem - --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,RuntimeClass,PodSecurityPolicy - --etcd-cafile=/var/lib/kubernetes/ca.pem - --etcd-certfile=/var/lib/kubernetes/kubernetes.pem - --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem - --etcd-servers=https://192.168.10.50:2379 - --event-ttl=1h - --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml - --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem - --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem - --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem - --kubelet-https=true - --runtime-config=authentication.k8s.io/v1beta1=true - --feature-gates=APIPriorityAndFairness=false - --service-account-key-file=/var/lib/kubernetes/service-account.pem - --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem - --service-account-issuer=api - --service-account-api-audiences=api - --service-cluster-ip-range=10.32.0.0/24 - --service-node-port-range=30000-32767 - --tls-cert-file=/var/lib/kubernetes/kubernetes.pem - --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem - --http2-max-streams-per-connection=3000 - --max-requests-inflight=3000 - --max-mutating-requests-inflight=1000 - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-apiserver CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 82c371fd9d99e 83e685a0b921ef5dd91eb3cdf208ba70690c1dd7decfc39bb3903be6ede752e6 24 seconds ago Running kube-apiserver 0 6af4d1b99fa37 default\u306ePodSecurityPolicy(PSP)\u3092\u4f5c\u6210\u3059\u308b staticPod \u3092\u4f5c\u6210\u3059\u308b\u969b\u306bkubelet\u304b\u3089mirror pod\u4f5c\u6210\u30ea\u30af\u30a8\u30b9\u30c8\u304c\u62d2\u5426\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3057\u307e\u3059 ( \u53c2\u8003 ) PSP / ClusterRole / ClusterRoleBinding cat << EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: annotations: apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default' name: default spec: allowedCapabilities: [] # default set of capabilities are implicitly allowed allowPrivilegeEscalation: false fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 hostIPC: true hostNetwork: true hostPID: true privileged: true readOnlyRootFilesystem: true runAsUser: rule: 'MustRunAsNonRoot' seLinux: rule: 'RunAsNonRoot' supplementalGroups: rule: 'RunAsNonRoot' ranges: # Forbid adding the root group. - min: 1 max: 65535 volumes: - 'configMap' - 'downwardAPI' - 'emptyDir' - 'persistentVolumeClaim' - 'projected' - 'secret' hostNetwork: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny' --- # Cluster role which grants access to the default pod security policy apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: default-psp rules: - apiGroups: - policy resourceNames: - default resources: - podsecuritypolicies verbs: - use --- # Cluster role binding for default pod security policy granting all authenticated users access apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: default-psp roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: default-psp subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:authenticated EOF $ cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - <\u7701\u7565> podsecuritypolicy.policy/default created clusterrole.rbac.authorization.k8s.io/default-psp created clusterrolebinding.rbac.authorization.k8s.io/default-psp created master node\u306bPod\u304cschedule\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u3059\u308b taint\u3092\u8a2d\u5b9a\u3059\u308b https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/ https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/ kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule $ kubectl get node k8s-master -o=jsonpath='{.spec.taints}' [{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\"}]","title":"\u624b\u9806"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#120","text":"failed creating mandatory flowcontrol settings: failed getting mandatory FlowSchema exempt due to the server was unable to return a response in the time allotted, but may still be processing the request \u89e3\u6c7a\u65b9\u6cd5(2021/04/17\u6642\u70b9) https://github.com/kubernetes/kubernetes/issues/97525#issuecomment-753022219 kube-apiserver\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u4ee5\u4e0b2\u3064\u3092\u4ed8\u52a0\u3059\u308b --feature-gates=APIPriorityAndFairness=false --runtime-config=flowcontrol.apiserver.k8s.io/v1beta1=false","title":"1.20\u306e\u30d0\u30b0?"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#kubeletwebhookkube-api-serverwebhook","text":"failed to run Kubelet: no client provided, cannot use webhook authentication \u89e3\u6c7a\u65b9\u6cd5 Webhook\u8a8d\u8a3c\u3092\u6709\u52b9\u306b\u3059\u308b https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/ https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication","title":"kubelet\u304cWebhook\u8a8d\u8a3c\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u306e\u306bkube-api-server\u3067Webhook\u8a8d\u8a3c\u304c\u6709\u52b9\u3067\u306a\u3044\u5834\u5408"},{"location":"setup/master/02_bootstrapping_kube-apiserver/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/ https://kubernetes.io/docs/reference/access-authn-authz/webhook/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/master/03_bootstrapping_kube-controller-manager/","text":"\u624b\u9806 Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << EOF > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo buildah bud -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: localhost/k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"100m\" memory: \"128Mi\" limits: cpu: \"300m\" memory: \"256Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=true - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb \u30a8\u30e9\u30fc\u4e8b\u4f8b Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044 \u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"04. bootstrapping kube-controller-manager"},{"location":"setup/master/03_bootstrapping_kube-controller-manager/#_1","text":"Dockerfile_kube-controller-manager.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-controller-manager.armhf cat << EOF > Dockerfile_kube-controller-manager.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-controller-manager \\ && chmod +x /usr/bin/kube-controller-manager \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY ca.pem \\ ca-key.pem \\ service-account-key.pem \\ kube-controller-manager.kubeconfig \\ /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-controller-manager\"] EOF image build sudo buildah bud -t k8s-kube-controller-manager --file=Dockerfile_kube-controller-manager.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b --allocate-node-cidrs=true Node resource\u306e spec.podCIDR \u3078CIDR\u304c\u8a2d\u5b9a\u3055\u308c\u308b kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}' spec.podCIDR \u306e\u5024\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044node instance\u3067\u306fCNI Plugin(flannel)\u304c\u6b63\u5e38\u52d5\u4f5c\u3057\u306a\u304b\u3063\u305f /etc/kubelet.d/kube-controller-manager.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-controller-manager.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-controller-manager namespace: kube-system labels: tier: control-plane component: kube-controller-manager spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-controller-manager image: localhost/k8s-kube-controller-manager:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"100m\" memory: \"128Mi\" limits: cpu: \"300m\" memory: \"256Mi\" command: - /usr/bin/kube-controller-manager - --bind-address=0.0.0.0 - --cluster-cidr=10.200.0.0/16 - --allocate-node-cidrs=true - --node-cidr-mask-size=24 - --cluster-name=kubernetes - --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem - --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem - --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig - --leader-elect=true - --root-ca-file=/var/lib/kubernetes/ca.pem - --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem - --service-cluster-ip-range=10.32.0.0/24 - --use-service-account-credentials=true - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-controller-manager CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a72cec7323686 4ada5d332b2c795b6333b8b6c538491dec96fb80f81b600359615651725b0ccf 20 seconds ago Running kube-controller-manager 0 526d7f2e9d3cb","title":"\u624b\u9806"},{"location":"setup/master/03_bootstrapping_kube-controller-manager/#_2","text":"","title":"\u30a8\u30e9\u30fc\u4e8b\u4f8b"},{"location":"setup/master/03_bootstrapping_kube-controller-manager/#clienttimeoutkube-control-managerkube-scheduler","text":"\u767a\u751f\u3057\u305f\u3089\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u518d\u8d77\u52d5\u3059\u308b\u3053\u3068\u3067\u56de\u5fa9\u3059\u308b kube-apiserver\u306b\u5bfe\u3059\u308b\u8ca0\u8377\u304c\u4e0a\u304c\u308b\u3068\u767a\u751f\u3057\u6613\u304f\u306a\u308b E0325 11:08:47.205570 1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Get \"https://192.168.10.50:6443/apis/coordination.k8s.io/v1/namespaces/kube- system/leases/kube-controller-manager?timeout=10s\": context deadline exceeded I0325 11:08:47.205695 1 leaderelection.go:278] failed to renew lease kube-system/kube-controller-manager: timed out waiting for the condition F0325 11:08:47.205929 1 controllermanager.go:294] leaderelection lost","title":"Client.Timeout\u3092\u8d85\u3048\u305f\u305f\u3081\u3001kube-control-manager\u3068kube-scheduler\u304c\u30ed\u30c3\u30af\u3092\u53d6\u5f97\u3067\u304d\u306a\u3044"},{"location":"setup/master/03_bootstrapping_kube-controller-manager/#_3","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/ node(flannel)\u306e Error registering network: failed to acquire lease: node \"k8s-node1\" pod cidr not assigned \u30a8\u30e9\u30fc\u306b\u95a2\u3057\u3066 https://blog.net.ist.i.kyoto-u.ac.jp/2019/11/06/kubernetes-%E6%97%A5%E8%A8%98-2019-11-05/ https://devops.stackexchange.com/questions/5898/how-to-get-kubernetes-pod-network-cidr","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/master/04_bootstrapping_kube-scheduler/","text":"\u624b\u9806 Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << EOF > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF image build sudo buildah bud -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: localhost/k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"100m\" memory: \"128Mi\" limits: cpu: \"300m\" memory: \"256Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1 \u53c2\u8003\u6587\u732e https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"05. bootstrapping kube-scheduler"},{"location":"setup/master/04_bootstrapping_kube-scheduler/#_1","text":"Dockerfile_kube-scheduler.armhf \u3092\u4f5c\u6210\u3059\u308b Dockerfile_kube-scheduler.armhf cat << EOF > Dockerfile_kube-scheduler.armhf FROM arm64v8/ubuntu:bionic RUN set -ex \\ && apt update \\ && apt install -y wget \\ && apt clean \\ && wget -P /usr/bin/ https://dl.k8s.io/v1.20.1/bin/linux/arm64/kube-scheduler \\ && chmod +x /usr/bin/kube-scheduler \\ && install -o root -g root -m 755 -d /var/lib/kubernetes \\ && install -o root -g root -m 755 -d /etc/kubernetes/config COPY kube-scheduler.yaml /etc/kubernetes/config/ COPY kube-scheduler.kubeconfig /var/lib/kubernetes/ ENTRYPOINT [\"/usr/bin/kube-scheduler\"] EOF kube-scheduler\u306econfig\u751f\u6210 k8s 1.19.0 \u3067 KubeSchedulerConfiguration \u304c beta\u306bupdate\u3055\u308c\u3066\u3044\u307e\u3059 https://qiita.com/everpeace/items/7dbf14773db82e765370 kube-scheduler.yaml cat << EOF > kube-scheduler.yaml --- apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"/var/lib/kubernetes/kube-scheduler.kubeconfig\" leaderElection: leaderElect: true EOF image build sudo buildah bud -t k8s-kube-scheduler --file=Dockerfile_kube-scheduler.armhf ./ pod manifests\u3092 /etc/kubelet.d \u3078\u4f5c\u6210\u3059\u308b /etc/kubelet.d/kube-scheduler.yaml cat << EOF | sudo tee /etc/kubelet.d/kube-scheduler.yaml --- apiVersion: v1 kind: Pod metadata: name: kube-scheduler namespace: kube-system labels: tier: control-plane component: kube-scheduler spec: # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ priorityClassName: system-node-critical hostNetwork: true containers: - name: kube-scheduler image: localhost/k8s-kube-scheduler:latest imagePullPolicy: IfNotPresent resources: requests: cpu: \"100m\" memory: \"128Mi\" limits: cpu: \"300m\" memory: \"256Mi\" command: - /usr/bin/kube-scheduler - --config=/etc/kubernetes/config/kube-scheduler.yaml - --v=2 EOF crictl \u3067\u30b3\u30f3\u30c6\u30ca\u8d77\u52d5\u3092\u78ba\u8a8d\u3059\u308b $ sudo crictl ps --name kube-scheduler CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID a19648dec2d54 70e852515b3c74175bb3ad4855287cb81101921b2b1f5a890fa4ebd0eeeee684 15 seconds ago Running kube-scheduler 0 da1d0572bc2b1","title":"\u624b\u9806"},{"location":"setup/master/04_bootstrapping_kube-scheduler/#_2","text":"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/","title":"\u53c2\u8003\u6587\u732e"},{"location":"setup/master/05_configuration_rbac_to_access_from-apiserver-to-kubelet/","text":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u624b\u9806 ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy) \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"06. kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/master/05_configuration_rbac_to_access_from-apiserver-to-kubelet/#kube-apiserver-kubelet","text":"kubectl \u3084\u4ed6Client tool\u3067\u306fkube-apiserver\u3078\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u6295\u3052\u307e\u3059\u3002 kube-apiserver \u3067\u306fetcd\u306b\u683c\u7d0d\u3055\u308c\u305f\u60c5\u5831\u3092\u57fa\u306b\u5404worker node(\u306e kubelet ) \u3068\u3084\u308a\u3068\u308a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002(\u4f8b\u3048\u3070exec,top,logs\u306a\u3069) kube-apiserver \u304b\u3089 kubelet \u306e\u5fc5\u8981\u306a\u30ea\u30bd\u30fc\u30b9\u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u9650\u3092\u4ed8\u4e0e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","title":"kube-apiserver \u304b\u3089 kubelet \u3078\u306e\u30a2\u30af\u30bb\u30b9\u6a29\u3092\u8a2d\u5b9a\u3059\u308b"},{"location":"setup/master/05_configuration_rbac_to_access_from-apiserver-to-kubelet/#_1","text":"ClusterRole system:kube-apiserver-to-kubelet \u3092\u4f5c\u6210 rbac.authorization.kubernetes.io/autoupdate annotations \u8d77\u52d5\u3059\u308b\u305f\u3073\u306b\u3001API\u30b5\u30fc\u30d0\u30fc\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRole\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308b\u6a29\u9650\u3067\u66f4\u65b0\u3057\u3001 \u30c7\u30d5\u30a9\u30eb\u30c8\u306eClusterRoleBinding\u3092\u4e0d\u8db3\u3057\u3066\u3044\u308bsubjects\u3067\u66f4\u65b0\u3057\u307e\u3059\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u8aa4\u3063\u305f\u5909\u66f4\u3092\u30af\u30e9\u30b9\u30bf\u304c\u4fee\u5fa9\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001 \u65b0\u3057\u3044Kubernetes\u30ea\u30ea\u30fc\u30b9\u3067\u6a29\u9650\u3068subjects\u304c\u5909\u66f4\u3055\u308c\u3066\u3082\u3001 Role\u3068RoleBinding\u3092\u6700\u65b0\u306e\u72b6\u614b\u306b\u4fdd\u3064\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 kubernetes.io/bootstrapping: rbac-defaults labels k8s\u306e\u65e2\u5b9a\u30af\u30e9\u30b9\u30bf\u30ed\u30fc\u30eb\u3068\u65e2\u5b9a\u30ed\u30fc\u30eb\u30d0\u30a4\u30f3\u30c9\u3067\u3042\u308b\u3053\u3068\u3092\u793a\u3059 cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:kube-apiserver-to-kubelet annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults rules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\" EOF Kubernetes \u30e6\u30fc\u30b6\u3078 system:kube-apiserver-to-kubelet ClusterRole\u3092\u7d10\u4ed8\u3051\u308b roleRef \u3067\u7d10\u4ed8\u3051\u305fRole\u3092\u6307\u5b9a\u3059\u308b subjects \u3067Role\u3092\u7d10\u4ed8\u3051\u308bAccount\u3092\u6307\u5b9a\u3059\u308b cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \"\" roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: Kubernetes EOF \u3053\u306e\u7d10\u4ed8\u3051\u304c\u6b63\u3057\u304f\u306a\u3044\u3001\u3082\u3057\u304f\u306f\u672a\u8a2d\u5b9a\u306e\u5834\u5408\u3001token\u4ed8\u304d\u3067kubectl\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u306b\u4ee5\u4e0b\u30a8\u30e9\u30fc\u3068\u306a\u308b Error from server (Forbidden): Forbidden (user=Kubernetes, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-proxy)","title":"\u624b\u9806"},{"location":"setup/master/05_configuration_rbac_to_access_from-apiserver-to-kubelet/#_2","text":"https://kubernetes.io/ja/docs/reference/access-authn-authz/rbac/ https://qiita.com/sheepland/items/67a5bb9b19d8686f389d","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"setup/master/06_health_check/","text":"health check \u624b\u9806 \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s master node\u4e0a\u306eresource\u78ba\u8a8d kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none> health checks kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b API endpoints for health kubectl get --raw='/readyz?verbose' Individual health checks kubectl get --raw='/livez/etcd' \u53c2\u8003\u8cc7\u6599 https://kubernetes.io/docs/reference/using-api/health-checks/","title":"07. health check"},{"location":"setup/master/06_health_check/#health-check","text":"","title":"health check"},{"location":"setup/master/06_health_check/#_1","text":"","title":"\u624b\u9806"},{"location":"setup/master/06_health_check/#_2","text":"kubectl get pods -n kube-system \u5b9f\u884c\u4f8b $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE etcd-k8s-master 1/1 Running 0 5m56s kube-apiserver-k8s-master 1/1 Running 0 6m7s kube-controller-manager-k8s-master 1/1 Running 0 4m2s kube-scheduler-k8s-master 1/1 Running 0 2m48s","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8d77\u52d5\u78ba\u8a8d"},{"location":"setup/master/06_health_check/#master-noderesource","text":"kubectl get nodes kubectl describe node <pod_name> \u5b9f\u884c\u4f8b $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready <none> 7m57s v1.20.1 $ kubectl describe node k8s-master Name: k8s-master Roles: <none> Labels: beta.kubernetes.io/arch=arm64 beta.kubernetes.io/os=linux kubernetes.io/arch=arm64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 17 Apr 2021 15:13:42 +0000 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master AcquireTime: <unset> RenewTime: Sat, 17 Apr 2021 16:34:29 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:41 +0000 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Sat, 17 Apr 2021 16:34:09 +0000 Sat, 17 Apr 2021 15:13:52 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.10.50 Hostname: k8s-master Capacity: cpu: 4 ephemeral-storage: 30459624Ki memory: 1892528Ki pods: 110 Allocatable: cpu: 3400m ephemeral-storage: 28071589432 memory: 1380528Ki pods: 110 System Info: Machine ID: 58f6de70444c4198b56b30122b6c77dc System UUID: 58f6de70444c4198b56b30122b6c77dc Boot ID: 79af3428-cf70-4189-a447-0b917a035a42 Kernel Version: 5.4.0-1032-raspi OS Image: Ubuntu 20.04.2 LTS Operating System: linux Architecture: arm64 Container Runtime Version: cri-o://1.20.2 Kubelet Version: v1.20.1 Kube-Proxy Version: v1.20.1 PodCIDR: 10.200.0.0/24 PodCIDRs: 10.200.0.0/24 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-apiserver-k8s-master 500m (14%) 1 (29%) 256Mi (18%) 384Mi (28%) 78m kube-system kube-controller-manager-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 76m kube-system kube-scheduler-k8s-master 100m (2%) 300m (8%) 128Mi (9%) 256Mi (18%) 75m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1200m (35%) 2600m (76%) memory 768Mi (56%) 1280Mi (94%) ephemeral-storage 0 (0%) 0 (0%) Events: <none>","title":"master node\u4e0a\u306eresource\u78ba\u8a8d"},{"location":"setup/master/06_health_check/#health-checks","text":"kube-apiserver\u306e\u8d77\u52d5\u30aa\u30d7\u30b7\u30e7\u30f3\u3067 --anonymous-auth=false \u3092\u4ed8\u52a0\u3057\u3066\u3044\u308b\u305f\u3081 https://localhost:6443 \u3078\u306eanonymous\u30a2\u30ab\u30a6\u30f3\u30c8\u3067\u306e\u78ba\u8a8d\u306f\u884c\u308f\u305a\u306b kubectl \u3067\u78ba\u8a8d\u3059\u308b","title":"health checks"},{"location":"setup/master/06_health_check/#api-endpoints-for-health","text":"kubectl get --raw='/readyz?verbose'","title":"API endpoints for health"},{"location":"setup/master/06_health_check/#individual-health-checks","text":"kubectl get --raw='/livez/etcd'","title":"Individual health checks"},{"location":"setup/master/06_health_check/#_3","text":"https://kubernetes.io/docs/reference/using-api/health-checks/","title":"\u53c2\u8003\u8cc7\u6599"}]}